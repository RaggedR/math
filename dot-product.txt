1) Dot product equivalence between x1 a1 + x2 a2 = |a| |x| cos(theta)
- requires the cosine rule for triangles
- higher dimensions project onto 2D plane

2) Definition of hyperplane A^T x = 0 defines points on a hyperplane
A^T x > 0 means x on same size as normal A
A^T x < 0 means x on opposite side from normal A
A is just a vector (not a matrix)
Hyperplane is subspace of dimension n-1. Defined by normal.
linearly separable hyperplane
bias, affine space

3) Perceptron convergence proof (see pdf)
Definition of perceptron
y=sign(wTxti)
Updating the weights
    If y!=ti​  (misclassified), update:
     w←w+nu ti ​xi
where nu is the learning rate.
Note that t_i (wT x + b) is always positive. ti = correct classification
Distance between a point and a plane
normalie |w*| = 1 (normalizing theoretical existing hyperplane)
gamma = minimum functional margin (perpendicular distance)
If the weight vector is normalized then minimum functional margin just the shortest perpendicular distance of any point to the hyperplane?

let w_k be the extimated plane after k updates. THis is the kth update from a "mistake" not k full sweeps through the data (epoch)

Cauchy Schwartz identity

Support vector machines
