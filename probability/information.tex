\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{booktabs}
\geometry{margin=1in}

\title{Why Expectation Is Blind to Dependence—but Variance Is Not}
\author{}
\date{}

\begin{document}

\maketitle

\Large

One of the most powerful and frequently used properties in probability theory is the \textbf{linearity of expectation}: for any random variables $X_1, X_2, \dots, X_n$,
\[
\mathbb{E}\!\left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \mathbb{E}[X_i].
\]
Remarkably, this identity holds \textit{regardless of whether the variables are independent, dependent, or even deterministically linked}. No assumptions about joint distributions are needed. This robustness makes expectation an indispensable tool in probabilistic analysis—especially when dealing with complex or unknown dependencies.

In stark contrast, the \textbf{variance of a sum} is highly sensitive to the relationships between variables. For two random variables $X$ and $Y$,
\[
\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2\,\mathrm{Cov}(X, Y),
\]
where $\mathrm{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$ is the covariance. Only when $\mathrm{Cov}(X, Y) = 0$—i.e., when $X$ and $Y$ are \textbf{uncorrelated}—does variance become additive. Since independence implies zero covariance (but not vice versa), independence is a sufficient—but not necessary—condition for variances to add.

This asymmetry arises because expectation is a \textit{linear} operator, while variance is \textit{quadratic}. The expectation of a sum depends only on marginal averages, but the variance of a sum involves the joint behavior through $\mathbb{E}[XY]$. Thus, while expectation ``ignores'' dependence, variance ``detects'' it—specifically, its linear component.

To illustrate this distinction concretely, consider the following natural example based on a simple random experiment.

\section*{A Coin-Toss Example: Dependent but Uncorrelated Variables}

Toss a fair coin twice. The sample space is
\[
\Omega = \{HH, HT, TH, TT\},
\]
with each outcome having probability $1/4$. Define two random variables:

\begin{itemize}
\item $X =$ (number of heads) $-$ (number of tails).  
    Thus, $X = +2$ for $HH$, $0$ for $HT$ or $TH$, and $-2$ for $TT$.
    
    \item $Y = \begin{cases}
        +1 & \text{if both tosses are the same (i.e., } HH \text{ or } TT\text{)}, \\
        -1 & \text{if the tosses differ (i.e., } HT \text{ or } TH\text{)}.
    \end{cases}$
\end{itemize}

The joint distribution is summarized below:


\begin{center}
\begin{tabular}{lccc}
\toprule
Outcome & $X$ & $Y$ & Probability \\
\midrule
$HH$ & $+2$ & $+1$ & $1/4$ \\
$HT$ & $0$  & $-1$ & $1/4$ \\
$TH$ & $0$  & $-1$ & $1/4$ \\
$TT$ & $-2$ & $+1$ & $1/4$ \\
\bottomrule
\end{tabular}
\end{center}

\subsection*{Dependence}
The variables are clearly dependent: if $Y = +1$ (tosses match), then $X$ must be $\pm 2$; if $Y = -1$ (tosses differ), then $X = 0$ with certainty. Formally,
\[
P(X = 0, Y = +1) = 0 \quad \text{but} \quad P(X = 0)P(Y = +1) = \frac{1}{2} \cdot \frac{1}{2} = \frac{1}{4} \ne 0,
\]
so $X$ and $Y$ are \textbf{not independent}.

\subsection*{Zero Correlation}
Now compute the covariance:
\[
\mathbb{E}[X] = (+2)\cdot\frac{1}{4} + 0\cdot\frac{1}{2} + (-2)\cdot\frac{1}{4} = 0,
\]
\[
\mathbb{E}[Y] = (+1)\cdot\frac{1}{2} + (-1)\cdot\frac{1}{2} = 0,
\]
\[
\mathbb{E}[XY] = (+2)(+1)\cdot\frac{1}{4} + (0)(-1)\cdot\frac{1}{4} + (0)(-1)\cdot\frac{1}{4} + (-2)(+1)\cdot\frac{1}{4} = \frac{2}{4} - \frac{2}{4} = 0.
\]
Hence,
\[
\mathrm{Cov}(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] = 0 - 0 = 0.
\]
So $X$ and $Y$ are \textbf{uncorrelated}, despite being dependent.

\section*{Implications for Sums}

Now consider the sum $S = X + Y$. By linearity of expectation—\textit{which requires no independence}—we have
\[
\mathbb{E}[S] = \mathbb{E}[X] + \mathbb{E}[Y] = 0 + 0 = 0.
\]

However, the variance of $S$ is
\[
\mathrm{Var}(S) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2\,\mathrm{Cov}(X, Y) = \mathrm{Var}(X) + \mathrm{Var}(Y),
\]
because $\mathrm{Cov}(X, Y) = 0$. In this case, variances \textit{do} add—but \textit{not because $X$ and $Y$ are independent} (they are not!), but because they happen to be uncorrelated.

This underscores a crucial point: **variance additivity depends on uncorrelatedness, not independence**. If we had chosen different functions of the coin tosses that were correlated, the covariance term would be nonzero, and the variance of the sum would reflect their interaction.

\section*{Conclusion}

Expectation is remarkably agnostic to dependence: the expected value of a sum is always the sum of expected values. Variance, however, encodes how variables fluctuate \textit{together}. While independence guarantees zero covariance, the converse is false—as our coin-toss example shows. Thus, when analyzing sums of random variables:

\begin{itemize}
\item Use linearity of expectation freely—even under complex dependence.
    \item Exercise caution with variance: always consider possible covariance.
\end{itemize}

This distinction is not merely technical; it shapes how we model uncertainty, design experiments, and interpret data in statistics, machine learning, and the physical sciences.



\section*{Dependence as Information Sharing (Shannon’s View)}

In classical probability, two random variables \(X\) and \(Y\) are \textbf{statistically independent} if their joint distribution factors:
\[
P(X = x, Y = y) = P(X = x) \cdot P(Y = y) \quad \text{for all } x, y.
\]
If this fails \textit{anywhere}, they are \textbf{dependent}.

But this is a \textit{mathematical} definition. The \textbf{philosophical and informational meaning}—thanks to \textbf{Claude Shannon’s information theory}—is more intuitive:

\begin{quote}
    \textbf{\(X\) and \(Y\) are dependent if observing one changes your knowledge (i.e., your probability assessment) about the other.}
\end{quote}

This change in knowledge is quantified as a \textbf{reduction in uncertainty}, and uncertainty is measured by \textbf{entropy}.

\section*{Entropy: The Measure of Uncertainty}

\begin{itemize}
\item The \textbf{entropy} of \(X\), denoted \(H(X)\), is the average uncertainty (in bits) before observing \(X\):
    \[
    H(X) = -\sum_x P(x) \log_2 P(x).
    \]
    High entropy = high unpredictability.
    
    \item The \textbf{conditional entropy} \(H(X \mid Y)\) is the average uncertainty about \(X\) \textit{after} observing \(Y\).
\end{itemize}

If \(X\) and \(Y\) are \textbf{independent}, then knowing \(Y\) tells you \textit{nothing} about \(X\), so:
\[
H(X \mid Y) = H(X).
\]

But if they are \textbf{dependent}, then:
\[
H(X \mid Y) < H(X).
\]
Your uncertainty about \(X\) \textbf{decreases} once you know \(Y\).

\section*{Mutual Information: The Shared Knowledge}

The amount of uncertainty reduced is called the \textbf{mutual information} between \(X\) and \(Y\):
\[
I(X; Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X).
\]

\begin{itemize}
\item \(I(X; Y) \geq 0\),
    \item \(I(X; Y) = 0\) \textbf{if and only if} \(X\) and \(Y\) are independent,
    \item Larger \(I(X; Y)\) means stronger dependence (more shared information).
\end{itemize}

Crucially, \textbf{mutual information captures \textit{any} kind of dependence}—linear, nonlinear, deterministic, or stochastic.

\section*{Back to Our Coin-Toss Example}

Recall:
\begin{itemize}
\item \(X =\) net heads minus tails,
    \item \(Y = +1\) if tosses match, \(-1\) if they differ.
\end{itemize}

We saw that \(\mathrm{Cov}(X, Y) = 0\), so they are \textbf{uncorrelated}.

But are they \textbf{informationally independent}?

No. Consider:
\begin{itemize}
\item Before seeing \(Y\), \(X\) could be \(-2, 0,\) or \(+2\).
    \item If you learn \(Y = -1\) (tosses differ), you know \textbf{with certainty} that \(X = 0\).
    \item Your uncertainty about \(X\) drops from \(H(X) > 0\) to \(H(X \mid Y = -1) = 0\).
\end{itemize}

Thus, \(H(X \mid Y) < H(X)\), so \(I(X; Y) > 0\).

\begin{center}
    \textbf{They share information \(\rightarrow\) they are dependent}, even though correlation is zero.
\end{center}

\section*{Philosophical Implications}

\begin{enumerate}
    \item \textbf{Knowledge Is Probabilistic} \\
    Learning isn’t about certainty—it’s about \textbf{updating beliefs}. Dependence means your belief about one variable \textit{should} change when you observe the other. Correlation misses this if the update isn’t linear.

    \item \textbf{Correlation Is a Narrow Channel} \\
    Mutual information measures \textit{total} shared information; correlation measures only the \textbf{linear component}. It’s like judging a symphony by its average pitch—you miss harmony, rhythm, and timbre.

    \item \textbf{Causality Often Hides in Nonlinear Dependence} \\
    Many causal mechanisms (e.g., thresholds, feedback loops, logical rules) create \textbf{nonlinear dependencies}. If we only test for correlation, we may conclude ``no link'' where a deep causal structure exists.

    \item \textbf{Science Requires Richer Tools} \\
    Relying solely on correlation encourages a \textbf{linear worldview}. But biology, economics, and social systems thrive on nonlinear interdependence. Information theory gives us a language to detect and quantify those links.
\end{enumerate}

\section*{A Deeper Unity}

Shannon’s insight reveals a profound unity:  
\begin{quote}
    \textbf{Statistical dependence = information flow.}
\end{quote}

This reframes probability not just as a calculus of chance, but as a \textbf{calculus of knowledge}. Every time two variables are dependent, there is a channel—however subtle—through which information passes.

Correlation is just one (limited) way to detect that channel. Mutual information, conditional entropy, and other information-theoretic tools let us \textbf{listen more carefully}.

\section*{In Summary}
\begin{itemize}

\item \textbf{Dependence} means: \textit{Knowing \(Y\) changes what you expect about \(X\).}
    \item This change is a \textbf{reduction in uncertainty}, measured by entropy.
    \item The amount of shared information is \textbf{mutual information} \(I(X;Y)\).
    \item \textbf{Correlation can be zero even when \(I(X;Y) > 0\)}—because correlation only sees linear patterns.
    \item Thus, \textbf{uncorrelated \(\ne\) independent}—not just mathematically, but \textbf{epistemologically}: the variables still ``speak'' to each other; we just need the right ears to hear it.
\end{itemize}

This is why modern data science increasingly turns to \textbf{information-theoretic methods} (like mutual information feature selection, entropy-based clustering, etc.)—to uncover the hidden conversations between variables that correlation silences.


Let’s compute the \textbf{entropy} and \textbf{mutual information} for the coin-toss example:

\begin{itemize}
\item Toss a fair coin twice.
    \item Define:
        \item \(X =\) (number of heads) \(-\) (number of tails) \(\rightarrow X \in \{-2, 0, +2\}\)
        \item \(Y = +1\) if tosses match (\(HH\) or \(TT\)), \(-1\) if they differ (\(HT\) or \(TH\))
\end{itemize}

From earlier, the joint distribution is:

\begin{center}
\begin{tabular}{lccc}
\toprule
Outcome & \(X\) & \(Y\) & Probability \\
\midrule
HH      & +2   & +1   & 1/4         \\
HT      & 0    & -1   & 1/4         \\
TH      & 0    & -1   & 1/4         \\
TT      & -2   & +1   & 1/4         \\
\bottomrule
\end{tabular}
\end{center}

\section*{Step 1: Marginal Distribution of \(X\)}

\begin{itemize}
\item \(P(X = -2) = P(TT) = 1/4\)
    \item \(P(X = 0) = P(HT \text{ or } TH) = 1/4 + 1/4 = 1/2\)
    \item \(P(X = +2) = P(HH) = 1/4\)
\end{itemize}

So:
\[
P_X(-2) = \frac{1}{4}, \quad P_X(0) = \frac{1}{2}, \quad P_X(+2) = \frac{1}{4}
\]

\textbf{Entropy of \(X\)}:
\[
H(X) = -\sum_x P(x) \log_2 P(x) = -\left[ \frac{1}{4} \log_2 \frac{1}{4} + \frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{4} \log_2 \frac{1}{4} \right]
\]

Compute:
\begin{itemize}
\item \(\log_2(1/4) = -2\)
    \item \(\log_2(1/2) = -1\)
\end{itemize}
So:
\[
H(X) = -\left[ \frac{1}{4}(-2) + \frac{1}{2}(-1) + \frac{1}{4}(-2) \right] = -\left[ -\frac{2}{4} - \frac{1}{2} - \frac{2}{4} \right] = -\left[ -0.5 - 0.5 - 0.5 \right] = -(-1.5) = 1.5 \text{ bits}
\]

\[
\boxed{H(X) = \frac{3}{2} \text{ bits}}
\]

\section*{Step 2: Marginal Distribution of \(Y\)}

\begin{itemize}
\item \(P(Y = +1) = P(HH \text{ or } TT) = 1/4 + 1/4 = 1/2\)
    \item \(P(Y = -1) = P(HT \text{ or } TH) = 1/2\)
\end{itemize}

So \(Y\) is Bernoulli(1/2) over \(\{-1, +1\}\).

\textbf{Entropy of \(Y\)}:
\[
H(Y) = -\left[ \frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{2} \log_2 \frac{1}{2} \right] = -\left[ -\frac{1}{2} - \frac{1}{2} \right] = 1 \text{ bit}
\]

\[
\boxed{H(Y) = 1 \text{ bit}}
\]

\section*{Step 3: Conditional Entropy \(H(X \mid Y)\)}

We compute \(H(X \mid Y = y)\) for each \(y\), then average.

\begin{itemize}
\item \textbf{When \(Y = +1\)} (probability 1/2):  
    Outcomes: \(HH\) → \(X = +2\), \(TT\) → \(X = -2\)  
    So \(P(X = +2 \mid Y = +1) = \frac{1/4}{1/2} = 1/2\),  
    \(P(X = -2 \mid Y = +1) = 1/2\),  
    \(P(X = 0 \mid Y = +1) = 0\)

    Entropy:
    \[
    H(X \mid Y = +1) = -\left[ \frac{1}{2} \log_2 \frac{1}{2} + \frac{1}{2} \log_2 \frac{1}{2} \right] = 1 \text{ bit}
    \]

    \item \textbf{When \(Y = -1\)} (probability 1/2):  
    Outcomes: \(HT, TH\) → both give \(X = 0\)  
    So \(P(X = 0 \mid Y = -1) = 1\)

    Entropy:
    \[
    H(X \mid Y = -1) = -[1 \cdot \log_2 1] = 0 \text{ bits}
    \]
\end{itemize}
Now average:
\[
H(X \mid Y) = P(Y = +1) H(X \mid Y = +1) + P(Y = -1) H(X \mid Y = -1) = \frac{1}{2}(1) + \frac{1}{2}(0) = 0.5 \text{ bits}
\]

\[
\boxed{H(X \mid Y) = \frac{1}{2} \text{ bit}}
\]

\section*{Step 4: Mutual Information \(I(X; Y)\)}

\[
I(X; Y) = H(X) - H(X \mid Y) = \frac{3}{2} - \frac{1}{2} = 1 \text{ bit}
\]

We can verify via \(H(Y) - H(Y \mid X)\):

\begin{itemize}
\item \textbf{When \(X = 0\)} (prob 1/2): then outcome is \(HT\) or \(TH\) → \(Y = -1\) with certainty → \(H(Y \mid X = 0) = 0\)
    \item \textbf{When \(X = +2\)} (prob 1/4): outcome is \(HH\) → \(Y = +1\) → \(H(Y \mid X = +2) = 0\)
    \item \textbf{When \(X = -2\)} (prob 1/4): outcome is \(TT\) → \(Y = +1\) → \(H(Y \mid X = -2) = 0\)
\end{itemize}

So \(H(Y \mid X) = 0\), and \(I(X; Y) = H(Y) - 0 = 1\) bit. \(\checkmark\) Consistent.

\[
\boxed{I(X; Y) = 1 \text{ bit}}
\]

\section*{Final Results}

\begin{itemize}
\item \(H(X) = 1.5\) bits  
    \item \(H(Y) = 1\) bit  
    \item \(H(X \mid Y) = 0.5\) bits  
    \item \(H(Y \mid X) = 0\) bits  
    \item \textbf{Mutual Information}: \(I(X; Y) = 1\) bit
\end{itemize}

\section*{Interpretation}

\begin{itemize}
\item Knowing \(Y\) reduces uncertainty about \(X\) by \textbf{1 bit} (from 1.5 to 0.5 bits).
    \item Knowing \(X\) \textbf{completely determines} \(Y\) (since if \(X = 0\), \(Y = -1\); if \(X = \pm 2\), \(Y = +1\)), so \(H(Y \mid X) = 0\).
    \item Despite \textbf{zero correlation}, there is \textbf{1 full bit of shared information}—a substantial dependence.
\end{itemize}

This quantifies the philosophical point: \textbf{uncorrelated does not mean unrelated}.









\section*{Mutual Information Example: Dice Roll and Sum}

Let \(X\) and \(Y\) be discrete random variables with joint distribution \(p(x,y)\), and marginals \(p(x)\), \(p(y)\).

\begin{enumerate}
    \item \textbf{Entropy} \\
    Measures the average uncertainty in a random variable:
    \[
    H(X) = -\sum_{x} p(x) \log_2 p(x)
    \]

    \item \textbf{Conditional Entropy} \\
    Average uncertainty in \(X\) given knowledge of \(Y\):
    \[
    H(X \mid Y) = -\sum_{y} p(y) \sum_{x} p(x \mid y) \log_2 p(x \mid y) 
    = \sum_{x,y} p(x,y) \log_2 \frac{1}{p(x \mid y)}
    \]

    \item \textbf{Mutual Information} \\
    Reduction in uncertainty about \(X\) due to knowing \(Y\) (symmetric):
    \[
    I(X;Y) = H(X) - H(X \mid Y) = H(Y) - H(Y \mid X)
    \]
    Equivalently, in terms of joint and marginals:
    \[
    I(X;Y) = \sum_{x,y} p(x,y) \log_2 \left( \frac{p(x,y)}{p(x)p(y)} \right)
    \]
\end{enumerate}
All quantities are measured in \textbf{bits} (logarithm base 2).

\section*{Natural Dice Example: One Die and the Sum of Two Dice}

\subsection*{Setup}
Roll two fair six-sided dice, denoted \(D_1\) and \(D_2\). Define:


\begin{itemize}
\item \(X = D_1\) (the outcome of the first die)
    \item \(Y = D_1 + D_2\) (the sum of both dice)
We compute \(I(X;Y)\): how much does knowing the sum tell us about the first die?
\end{itemize}
This is a natural dependence: for example, if \(Y = 2\), then \(X\) must be 1; if \(Y = 7\), then \(X\) could be any value from 1 to 6.

\subsection*{Joint and Marginal Distributions}

Since the dice are independent and fair,
\[
P(D_1 = x, D_2 = d) = \frac{1}{36}, \quad x,d \in \{1,\dots,6\}.
\]
Because \(Y = X + D_2\), we have
\[
p(x, y) = P(X = x, Y = y) =
\begin{cases}
\frac{1}{36} & \text{if } 1 \le x \le 6 \text{ and } 1 \le y - x \le 6, \\
0 & \text{otherwise}.
\end{cases}
\]

The marginal of \(X\) is uniform:
\[
p(x) = \frac{1}{6}, \quad x = 1,\dots,6.
\]

The marginal of \(Y\) (sum of two dice) is well known:
\begin{center}
\begin{tabular}{c|ccccccccccc}
\(y\) & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
\midrule
\(p(y)\) & \(\frac{1}{36}\) & \(\frac{2}{36}\) & \(\frac{3}{36}\) & \(\frac{4}{36}\) & \(\frac{5}{36}\) & \(\frac{6}{36}\) & \(\frac{5}{36}\) & \(\frac{4}{36}\) & \(\frac{3}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\)
\end{tabular}
\end{center}

\subsection*{Computing Mutual Information}

We use the identity:
\[
I(X;Y) = H(X) - H(X \mid Y).
\]

\subsubsection*{Entropy of \(X\)}
Since \(X\) is uniform over 6 outcomes,
\[
H(X) = \log_2 6 \approx 2.58496 \text{ bits}.
\]

\subsubsection*{Conditional Entropy \(H(X \mid Y)\)}

Given \(Y = y\), the possible values of \(X\) are those for which \(1 \le y - x \le 6\), i.e., \(x \in [\max(1, y-6), \min(6, y-1)]\). The number of such values is:
\[
n_y = 
\begin{cases}
1 & y = 2 \text{ or } 12, \\
2 & y = 3 \text{ or } 11, \\
3 & y = 4 \text{ or } 10, \\
4 & y = 5 \text{ or } 9, \\
5 & y = 6 \text{ or } 8, \\
6 & y = 7.
\end{cases}
\]

Because all valid \((x,y)\) pairs have equal probability (\(1/36\)), the conditional distribution \(p(x \mid y)\) is uniform over the \(n_y\) possibilities. Hence,
\[
H(X \mid Y = y) = \log_2 n_y.
\]

Therefore,
\[
H(X \mid Y) = \sum_{y=2}^{12} p(y) \log_2 n_y.
\]

Grouping symmetric terms:
\[
\begin{aligned}
H(X \mid Y) &= \frac{2}{36} \cdot \log_2 1 
+ \frac{4}{36} \cdot \log_2 2 
+ \frac{6}{36} \cdot \log_2 3 
+ \frac{8}{36} \cdot \log_2 4 \\
&\quad + \frac{10}{36} \cdot \log_2 5 
+ \frac{6}{36} \cdot \log_2 6.
\end{aligned}
\]

Using \(\log_2 1 = 0\), \(\log_2 2 = 1\), \(\log_2 4 = 2\), and numerical values:
\[
\begin{aligned}
\log_2 3 &\approx 1.58496, \\
\log_2 5 &\approx 2.32193, \\
\log_2 6 &\approx 2.58496,
\end{aligned}
\]
we compute:
\[
\begin{aligned}
H(X \mid Y) &\approx 
\frac{4}{36}(1) 
+ \frac{6}{36}(1.58496) 
+ \frac{8}{36}(2) 
+ \frac{10}{36}(2.32193) 
+ \frac{6}{36}(2.58496) \\
&= \frac{4}{36} + \frac{9.5098}{36} + \frac{16}{36} + \frac{23.2193}{36} + \frac{15.5098}{36} \\
&= \frac{68.2389}{36} \approx 1.8955 \text{ bits}.
\end{aligned}
\]

\subsubsection*{Mutual Information}

Finally,
\[
I(X;Y) = H(X) - H(X \mid Y) \approx 2.58496 - 1.8955 = \boxed{0.6895 \text{ bits}}.
\]

\section*{Interpretation}

- Initially, the first die has \(H(X) \approx 2.585\) bits of uncertainty.
- After observing the sum \(Y\), uncertainty reduces to \(H(X \mid Y) \approx 1.896\) bits.
- Thus, the sum reveals approximately \(0.69\) bits of information about the first die.
- This reflects intuition: extreme sums (e.g., 2 or 12) fully determine \(X\), while moderate sums (e.g., 7) leave significant uncertainty.

This example uses only standard, fair dice—no hidden variables or artificial constructions—and illustrates how mutual information quantifies dependence in a natural probabilistic setting.




\section*{Mutual Information and Bayes' Theorem: \\ Information Gain from Bayesian Updating}

Mutual information and Bayes' theorem are deeply connected:
\begin{itemize}
\item \textbf{Bayes' theorem} tells you how to update a \emph{single prior} \(P(X)\) to a \emph{posterior} \(P(X \mid Y = y)\) after observing a specific outcome \(Y = y\).
    \item \textbf{Mutual information} \(I(X;Y)\) tells you the \emph{expected reduction in uncertainty} about \(X\) \emph{on average} over all possible outcomes \(y\), weighted by their likelihood.
\end{itemize}


In short:
\begin{center}
    \textbf{Mutual information = Expected information gain from Bayesian updating.}
\end{center}

\section*{Formal Connection}

\subsection*{1. Bayes' Theorem and Information Gain for a Single Observation}

Given a prior distribution \(P(X)\), observing \(Y = y\) yields the posterior via Bayes' rule:
\[
P(X \mid Y = y) = \frac{P(Y = y \mid X)\, P(X)}{P(Y = y)}.
\]

The \textbf{information gained} from this observation is measured by the Kullback–Leibler (KL) divergence from the prior to the posterior:
\[
D_{\mathrm{KL}}\big(P(X \mid Y = y) \,\|\, P(X)\big) 
= \sum_{x} P(x \mid y) \log_2 \frac{P(x \mid y)}{P(x)}.
\]
This quantifies how much the belief about \(X\) changed due to seeing \(Y = y\).

\subsection*{2. Mutual Information as Expected KL Divergence}

Mutual information is the expectation of this KL divergence over all possible outcomes \(y\):
\[
\boxed{
I(X;Y) = \mathbb{E}_{Y}\!\left[ D_{\mathrm{KL}}\big(P(X \mid Y) \,\|\, P(X)\big) \right]
= \sum_{y} P(y) \sum_{x} P(x \mid y) \log_2 \frac{P(x \mid y)}{P(x)}.
}
\]

Using \(P(x,y) = P(x \mid y) P(y)\), this is algebraically equivalent to the standard definition:
\[
I(X;Y) = \sum_{x,y} P(x,y) \log_2 \left( \frac{P(x,y)}{P(x) P(y)} \right).
\]

Thus, mutual information is precisely the \textbf{average information gain} from applying Bayes' rule.

\section*{Interpretation}


[leftmargin=*]\begin{itemize}
    \item \textbf{KL divergence} \(D_{\mathrm{KL}}(P(X|y) \| P(X))\): \\
    ``How many extra bits would I waste if I encoded \(X\) using the prior instead of the correct posterior?'' \\
    -> This is the \emph{information gained} from observing \(Y = y\).

    \item \textbf{Mutual information} \(I(X;Y)\): \\
    ``On average, how many bits do I gain about \(X\) per observation of \(Y\)?'' \\
    -> This is the \emph{expected information gain} from Bayesian updating.
\end{itemize}

\section*{Concrete Dice Example: Connecting Bayes + Mutual Information}

Recall the setup:
\begin{itemize}
\item Roll two fair dice: \(D_1, D_2\).
    \item Let \(X = D_1\) (first die), \(Y = D_1 + D_2\) (sum).
    \item Prior: \(P(X = x) = \frac{1}{6}\) for \(x = 1,\dots,6\).
\end{itemize}

\subsection*{Bayesian Updating for Specific Observations}

\begin{enumerate}
    \item \textbf{Observe \(Y = 2\)}: \\
    Only possible if \(D_1 = 1, D_2 = 1\). \\
    Posterior: \(P(X = 1 \mid Y = 2) = 1\), others 0. \\
    Information gain:
    \[
    D_{\mathrm{KL}}(P(X|2) \| P(X)) = \log_2 \frac{1}{1/6} = \log_2 6 \approx 2.585 \text{ bits}.
    \]

    \item \textbf{Observe \(Y = 7\)}: \\
    All pairs \((1,6), (2,5), \dots, (6,1)\) equally likely. \\
    Posterior: \(P(X = x \mid Y = 7) = \frac{1}{6}\) for all \(x\). \\
    Information gain:
    \[
    D_{\mathrm{KL}}(P(X|7) \| P(X)) = \sum_{x=1}^6 \frac{1}{6} \log_2 \frac{1/6}{1/6} = 0 \text{ bits}.
    \]
\end{enumerate}

\subsection*{Mutual Information as the Average Gain}

Mutual information averages these gains over all possible sums:
\[
I(X;Y) = \sum_{y=2}^{12} P(Y = y) \cdot D_{\mathrm{KL}}\big(P(X \mid Y = y) \,\|\, P(X)\big).
\]

Using the known distribution of \(Y\):
\begin{center}
\begin{tabular}{c|ccccccccccc}
\(y\) & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
\midrule
\(P(Y=y)\) & \(\frac{1}{36}\) & \(\frac{2}{36}\) & \(\frac{3}{36}\) & \(\frac{4}{36}\) & \(\frac{5}{36}\) & \(\frac{6}{36}\) & \(\frac{5}{36}\) & \(\frac{4}{36}\) & \(\frac{3}{36}\) & \(\frac{2}{36}\) & \(\frac{1}{36}\) \\
\(n_y\) (support size) & 1 & 2 & 3 & 4 & 5 & 6 & 5 & 4 & 3 & 2 & 1 \\
\(D_{\mathrm{KL}}\) & \(\log_2 6\) & \(\log_2 3\) & \(\log_2 2\) & \(\log_2 \tfrac{4}{3}\)? & \dots & 0 & \dots & & & &
\end{tabular}
\end{center}

More precisely, since \(P(X \mid Y = y)\) is uniform over \(n_y\) values,
\[
D_{\mathrm{KL}}\big(P(X \mid Y = y) \,\|\, P(X)\big) = \log_2 n_y - \log_2 6 = \log_2 \left( \frac{n_y}{6} \right) \quad \text{(but note: actually } = \log_2 n_y \text{ because prior entropy cancels in expectation)}.
\]

Carrying out the full calculation (as in the earlier example) yields:
\[
I(X;Y) \approx 0.6895 \text{ bits}.
\]

This is exactly the \textbf{expected information gain} from observing the sum and applying Bayes' rule.

\section*{Why This Matters}

[leftmargin=*]\begin{itemize}
    \item \textbf{Bayesian inference}: Every Bayes update provides information; mutual information quantifies its average value.
    \item \textbf{Experimental design}: Choose observations \(Y\) that maximize \(I(X;Y)\) to learn most about \(X\).
    \item \textbf{Machine learning}: In variational inference, mutual information appears in bounds on model evidence.
    \item \textbf{Cognitive science}: Models perception as Bayesian updating, with mutual information measuring sensory informativeness.
\end{itemize}

\section*{Summary}

\begin{center}
\begin{tabular}{ll}
\toprule
Concept & Role \\
\midrule
Bayes' theorem & Updates prior \(P(X)\) → posterior \(P(X \mid Y = y)\) for a \emph{specific} \(y\). \\
KL divergence \(D_{\mathrm{KL}}(P(X|y) \| P(X))\) & Information gained from that \emph{specific} update. \\
Mutual information \(I(X;Y)\) & \emph{Expected} information gain over all \(y\). \\
\bottomrule
\end{tabular}
\end{center}

Thus, mutual information provides an \textbf{information-theoretic foundation for Bayesian learning}: it measures how much an observation is expected to teach us about the world.





\end{document}
