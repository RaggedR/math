\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{setspace}
\onehalfspacing

\title{Central Limit Theorem Proof Explained in Plain English}
\author{}
\date{}

\begin{document}

\maketitle
\Large

\section*{What We're Trying to Prove}

Imagine you have a bunch of random measurements---maybe people's heights, test scores, or coin flip results. Each measurement comes from the same underlying process, so they all have the same average (mean) and the same amount of spread (variance).

Now, if you take the average of many such measurements, something remarkable happens: \textbf{the distribution of this average becomes more and more like a bell curve (Gaussian distribution)}, no matter what the original distribution looked like!

The Central Limit Theorem makes this precise: if you properly adjust (standardize) your average by subtracting the true mean and dividing by the appropriate amount of spread, then as you take more and more measurements, this standardized average follows a standard normal distribution.

\section*{The Mathematical Setup}

Let's say we have random variables $X_1, X_2, X_3, \dots$ that are:
\begin{itemize}
\item \textbf{Independent}: Each measurement doesn't affect the others
    \item \textbf{Identically distributed}: They all come from the same underlying process
    \item \textbf{Finite mean $\mu$}: They have a well-defined average
    \item \textbf{Finite variance $\sigma^2$}: They don't have infinite spread
\end{itemize}
We want to study the standardized sum:
\[
Z_n = \frac{(X_1 + X_2 + \dots + X_n) - n\mu}{\sigma\sqrt{n}}
\]

This formula does two things:
\begin{enumerate}
    \item \textbf{Centers} the sum by subtracting the expected total ($n\mu$)
    \item \textbf{Scales} it by dividing by $\sigma\sqrt{n}$, which is the standard deviation of the sum
\end{enumerate}

\section*{Why Use Characteristic Functions?}

A \textbf{characteristic function} is like a ``fingerprint'' of a probability distribution. Every distribution has a unique characteristic function, and if two distributions have the same characteristic function, they're the same distribution.

The key insight is this: \textbf{instead of trying to prove that the distributions become Gaussian directly, we can prove that their ``fingerprints'' (characteristic functions) become the fingerprint of a Gaussian distribution}.

This is much easier because characteristic functions turn complicated operations (like adding random variables) into simple multiplication.

\section*{Step-by-Step Explanation}

\subsection*{Step 1: Simplify by Centering}

First, let's make our lives easier by working with centered variables. Define:
\[
Y_i = X_i - \mu
\]
Now each $Y_i$ has mean 0 and variance $\sigma^2$. Our standardized sum becomes:
\[
Z_n = \frac{Y_1 + Y_2 + \dots + Y_n}{\sigma\sqrt{n}}
\]

\subsection*{Step 2: Write Down the Characteristic Function}

The characteristic function of $Z_n$ is:
\[
\varphi_{Z_n}(t) = \mathbb{E}\left[e^{itZ_n}\right] = \mathbb{E}\left[\exp\left(it \cdot \frac{Y_1 + \dots + Y_n}{\sigma\sqrt{n}}\right)\right]
\]

Because the $Y_i$ are independent, we can separate this expectation into a product:
\[
\varphi_{Z_n}(t) = \left[\mathbb{E}\left[\exp\left(\frac{itY_1}{\sigma\sqrt{n}}\right)\right]\right]^n = \left[\varphi_{Y_1}\left(\frac{t}{\sigma\sqrt{n}}\right)\right]^n
\]

This is the magic of independence---adding independent random variables corresponds to multiplying their characteristic functions.

\subsection*{Step 3: Understand What Happens for Large n}

Here's the crucial observation: as $n$ gets very large, the argument $\frac{t}{\sigma\sqrt{n}}$ becomes very small (close to 0). So we only need to understand how the characteristic function of $Y_1$ behaves \textbf{near zero}.

\subsection*{Step 4: Expand Near Zero Using Taylor Series}

Think of the characteristic function $\varphi_{Y_1}(s) = \mathbb{E}[e^{isY_1}]$ as a smooth curve. Near $s = 0$, we can approximate it using its Taylor series (like approximating a curve with a polynomial near a point).

Using Euler's formula $e^{ix} = \cos x + i\sin x$ and taking expectations:
\[
\varphi_{Y_1}(s) = \mathbb{E}[\cos(sY_1)] + i\mathbb{E}[\sin(sY_1)]
\]

For small $s$, we can use the approximations:
\begin{itemize}
\item $\cos(sY_1) \approx 1 - \frac{(sY_1)^2}{2}$
    \item $\sin(sY_1) \approx sY_1 - \frac{(sY_1)^3}{6}$
\end{itemize}

Taking expectations and remembering that $\mathbb{E}[Y_1] = 0$ and $\mathbb{E}[Y_1^2] = \sigma^2$:
\[
\varphi_{Y_1}(s) \approx 1 - \frac{s^2\sigma^2}{2} + \text{(higher order terms)}
\]

The key point is that \textbf{the behavior near zero only depends on the first two moments} (mean and variance) of the distribution. All the details about the original shape of the distribution are hidden in the ``higher order terms'' that become negligible.

\subsection*{Step 5: Plug in the Small Argument}

Now substitute $s = \frac{t}{\sigma\sqrt{n}}$:
\[
\varphi_{Y_1}\left(\frac{t}{\sigma\sqrt{n}}\right) \approx 1 - \frac{1}{2}\left(\frac{t}{\sigma\sqrt{n}}\right)^2\sigma^2 = 1 - \frac{t^2}{2n}
\]

Notice how beautiful this is: the $\sigma^2$ cancels out, leaving us with something that doesn't depend on the original variance at all!

\subsection*{Step 6: Raise to the nth Power}

Now remember we need to raise this to the $n$th power:
\[
\varphi_{Z_n}(t) \approx \left(1 - \frac{t^2}{2n}\right)^n
\]

\subsection*{Step 7: Take the Limit}

Here's where calculus comes in. There's a famous limit from calculus:
\[
\lim_{n \to \infty} \left(1 + \frac{a}{n}\right)^n = e^a
\]

In our case, $a = -\frac{t^2}{2}$, so:
\[
\lim_{n \to \infty} \varphi_{Z_n}(t) = e^{-t^2/2}
\]

\subsection*{Step 8: Recognize the Gaussian Fingerprint}

The function $e^{-t^2/2}$ is exactly the characteristic function of the \textbf{standard normal distribution}! This is the ``fingerprint'' we were looking for.

\section*{Why This Proof is So Powerful}

This proof reveals something profound: \textbf{the Gaussian distribution is universal because it's what you get when you only care about the first two moments (mean and variance) of a distribution}.

When you add up many independent random variables:
\begin{itemize}
\item The details of their individual distributions get ``washed out''

    \item Only their means and variances matter for the limiting behavior
    \item The mathematical structure forces the result to be Gaussian
\end{itemize}

It's like mixing many different colors of paint---if you mix enough different colors together, you always get a muddy brown, regardless of what specific colors you started with. Similarly, if you add enough independent random effects together, you always get something that looks Gaussian.

\section*{The Big Picture}

The characteristic function approach works because it transforms a difficult problem about distributions into an easier problem about functions. By focusing on the behavior near zero (which corresponds to the large-scale behavior of the sum), we can ignore all the complicated details of the original distribution and see the universal Gaussian pattern emerge.

This is why the Central Limit Theorem appears everywhere in nature, science, and engineering---any phenomenon that results from the accumulation of many small, independent influences will naturally follow a bell curve, and this proof shows us exactly why that happens mathematically.



\section*{1. What is the Binomial Distribution?}

Imagine you’re flipping a (possibly biased) coin $n$ times. Each flip is a \textbf{Bernoulli trial}:
\begin{itemize}\item It has two outcomes: \textbf{success} (e.g., heads) with probability $p$, and \textbf{failure} (tails) with probability $1 - p$.
    \item All flips are \textbf{independent}: the result of one doesn’t affect the others.
\end{itemize}
Let $X$ be the total number of successes in those $n$ flips. Then $X$ follows a \textbf{binomial distribution}, written as:
\[
X \sim \operatorname{Binomial}(n, p).
\]

Its probability mass function is:
\[
\mathbb{P}(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}, \quad \text{for } k = 0, 1, \dots, n.
\]
This tells us how likely it is to get exactly $k$ heads in $n$ flips.

\section*{2. What Does ``Converges to the Gaussian'' Mean?}

As $n$ gets very large, the shape of the binomial distribution starts to look more and more like a \textbf{bell curve}—the famous \textbf{Gaussian (normal) distribution}.

But we must be careful: the binomial is \textbf{discrete} (only defined at integer values), while the Gaussian is \textbf{continuous}. So we don’t mean they become \textit{identical}, but rather that \textbf{after appropriate centering and scaling}, the binomial’s shape becomes indistinguishable from a normal curve.

This is where the \textbf{Central Limit Theorem (CLT)} comes in.

\section*{3. The Central Limit Theorem (CLT): The Big Idea}

The CLT is one of the most powerful results in probability. In simple terms, it says:

\begin{quote}
    If you add up a large number of independent, identically distributed (i.i.d.) random variables—each with finite mean and variance—then the sum (properly normalized) will look approximately like a normal distribution, no matter what the original distribution was!
\end{quote}

More formally, suppose $Y_1, Y_2, \dots, Y_n$ are i.i.d.\ random variables with:
\[
\mathbb{E}[Y_i] = \mu, \quad \operatorname{Var}(Y_i) = \sigma^2 < \infty.
\]
Define their sum: $S_n = Y_1 + Y_2 + \cdots + Y_n$.

Then the \textbf{standardized version} of this sum:
\[
Z_n = \frac{S_n - n\mu}{\sigma \sqrt{n}}
\]
\textbf{converges in distribution} to a \textbf{standard normal random variable} as $n \to \infty$. That is:
\[
Z_n \xrightarrow{d} \mathcal{N}(0, 1).
\]
This means the cumulative distribution function (CDF) of $Z_n$ gets closer and closer to the CDF of a standard normal.

\section*{4. Connecting the Binomial to the CLT}

Here’s the key insight:

\begin{center}
    \textbf{A binomial random variable is just the sum of $n$ independent Bernoulli random variables.}
\end{center}

Define $Y_i$ as the outcome of the $i$-th coin flip:
\[
Y_i = 
\begin{cases}
1 & \text{with probability } p \quad \text{(success)}, \\
0 & \text{with probability } 1 - p \quad \text{(failure)}.
\end{cases}
\]
Each $Y_i \sim \operatorname{Bernoulli}(p)$, and they’re independent.

Then the total number of successes is:
\[
X = Y_1 + Y_2 + \cdots + Y_n,
\]
so indeed $X \sim \operatorname{Binomial}(n, p)$.

Now compute the mean and variance of each $Y_i$:
\[
\mathbb{E}[Y_i] = p, \quad \operatorname{Var}(Y_i) = p(1 - p).
\]
Therefore, for the sum $X$:
\[
\mathbb{E}[X] = np, \quad \operatorname{Var}(X) = np(1 - p).
\]

\section*{5. Apply the CLT to the Binomial}

We now plug the binomial into the CLT framework.

Standardize $X$ as the CLT prescribes:
\[
Z_n = \frac{X - \mathbb{E}[X]}{\sqrt{\operatorname{Var}(X)}} = \frac{X - np}{\sqrt{np(1 - p)}}.
\]

According to the CLT, as $n \to \infty$, this standardized variable converges in distribution to a standard normal:
\[
Z_n \xrightarrow{d} \mathcal{N}(0, 1).
\]

In words:
\begin{quote}
    If you take a binomial count, subtract its average ($np$), and divide by its standard deviation ($\sqrt{np(1-p)}$), then for large $n$, the result behaves like a standard normal random variable.
\end{quote}

\section*{6. What Does This Mean Visually?}

Imagine plotting the binomial probabilities $\mathbb{P}(X = k)$ for $k = 0, 1, \dots, n$. For small $n$, the distribution may look lopsided or jagged—especially if $p$ is far from $0.5$.

But as $n$ grows:
\begin{itemize}\item The distribution becomes \textbf{smoother} (more possible values of $k$).
    \item Its shape becomes \textbf{symmetric and bell-shaped}, centered at $np$, with spread proportional to $\sqrt{n}$.
\end{itemize}
If you rescale the horizontal axis using
\[
z = \frac{k - np}{\sqrt{np(1 - p)}},
\]
then the histogram of these rescaled points will hug the standard normal curve tighter and tighter as $n$ increases.

This result is historically known as the \textbf{de Moivre–Laplace theorem}, a special case of the CLT for binomial distributions.

\section*{7. A Note on ``Convergence in Distribution''}

When we write $Z_n \xrightarrow{d} \mathcal{N}(0,1)$, we mean that for any real number $z$,
\[
\mathbb{P}(Z_n \leq z) \longrightarrow \Phi(z) \quad \text{as } n \to \infty,
\]
where $\Phi(z)$ is the CDF of the standard normal distribution.

This does \textit{not} mean that individual point probabilities $\mathbb{P}(X = k)$ become equal to the normal density—but it \textit{does} mean that \textbf{probabilities over intervals} (e.g., $\mathbb{P}(a \leq X \leq b)$) can be well-approximated by the area under the normal curve after standardizing.

Hence, in practice, we use the \textbf{normal approximation to the binomial}:
\[
\mathbb{P}(a \leq X \leq b) \approx \Phi\!\left( \frac{b + 0.5 - np}{\sqrt{np(1 - p)}} \right) - \Phi\!\left( \frac{a - 0.5 - np}{\sqrt{np(1 - p)}} \right),
\]
where the $\pm 0.5$ is the \textbf{continuity correction}, accounting for the fact that we’re approximating a discrete distribution with a continuous one.

\section*{8. Why Does This Happen? Intuition}

Think of each coin flip as a tiny random ``push''—sometimes $+1$ (success), sometimes $0$ (failure). When you add up thousands of these tiny, independent pushes, the randomness tends to \textbf{average out}, and the total fluctuation around the mean becomes \textbf{smooth and symmetric}.

Extreme outcomes (like all heads or all tails) become astronomically unlikely, while outcomes near the average dominate. The mathematics of summing independent noise naturally leads to the bell shape—this is the essence of the CLT.

Since the binomial is just a sum of simple yes/no trials, it’s a perfect candidate for this averaging effect.

\section*{9. Summary in One Sentence}

Because a binomial random variable is the sum of many independent, identical Bernoulli trials, the Central Limit Theorem guarantees that—when properly centered and scaled—it converges in distribution to a Gaussian (normal) random variable as the number of trials goes to infinity.

\section*{Final Thought}

This convergence is not just theoretical—it’s \textbf{practically useful}. It lets statisticians approximate binomial probabilities (which can be hard to compute for large $n$) using the well-understood normal distribution. And it’s a beautiful example of how \textbf{order emerges from randomness} when you look at large systems.








\end{document}
