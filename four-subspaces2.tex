\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{booktabs}


\newcommand{\nul}{\operatorname{Nul}}
\newcommand{\col}{\operatorname{Col}}
\newcommand{\row}{\operatorname{Row}}

\geometry{margin=1in}
\titleformat{\section}{\large\bfseries}{}{0em}{}
\titleformat{\subsection}{\bfseries}{}{0em}{}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\title{The Four Fundamental Subspaces and Their Orthogonality Relations}
\author{}
\date{}

\begin{document}

\maketitle

\Large

\section*{The goal of this section}

The goal of this section is to understand the \emph{four fundamental subspaces} associated with a finite dimensional matrix $A$ over the real numbers $\mathbb{R}$. We begin by reviewing matrix multiplication, then define our four subspaces and prove our main theorem. Next we introduce the dual space and adjoint operators. We define the annihilator of a subspace and give an alternative proof of the main theorem. Finally we prove the rank-nullity theorem and also prove that the rank of the transpose of a matrix is equal to the rank of the original matrix.

\section*{Matrix Multiplication}

Let us recall briefly the formula for the multiplication of a matrix by a vector. If $B v = w$ then $w_k = \sum_j B_{kj} v_j$. This can be interpreted in two different ways. Firstly as the dot product of each of the rows of $B$ with the vector $v$.


\[
\begin{array}{c@{\,}c}
\begin{bmatrix}
- & r_1 & - \\
- & r_2 & - \\
- & r_3 & - \\
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
v_3 \\
\end{bmatrix}
\end{array}
=
\begin{bmatrix}
r_1 . v \\
r_2 . v \\
r_3 . v \\
\end{bmatrix}
\]

Secondly as a linear combination of the columns.

\[
\begin{array}{c@{\,}c}
\begin{bmatrix}
| & | & | \\
c_1 & c_2 & c_3 \\
| & | & | \\
\end{bmatrix}
\begin{bmatrix}
v_1 \\
v_2 \\
v_3 \\
\end{bmatrix}
\end{array}
=
v_1 \begin{bmatrix}
| \\
c_1 \\
| \\
\end{bmatrix}
+
v_2 \begin{bmatrix}
| \\
c_2 \\
| \\
\end{bmatrix}
+
v_3 \begin{bmatrix}
| \\
c_3 \\
| \\
\end{bmatrix}
\]

It is worth pausing here for a moment to make sure you are clear on this. Here is an example:

\section*{Example}

Let
\[
B = \begin{bmatrix}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{bmatrix}, \qquad
\mathbf{v} = \begin{bmatrix} 2 \\ -1 \\ 1 \end{bmatrix}.
\]

Denote the rows of $B$ by
\[
r_1 = \begin{bmatrix} 1 & 2 & 3 \end{bmatrix}, \quad
r_2 = \begin{bmatrix} 4 & 5 & 6 \end{bmatrix}, \quad
r_3 = \begin{bmatrix} 7 & 8 & 9 \end{bmatrix},
\]
and the columns by
\[
c_1 = \begin{bmatrix} 1 \\ 4 \\ 7 \end{bmatrix}, \quad
c_2 = \begin{bmatrix} 2 \\ 5 \\ 8 \end{bmatrix}, \quad
c_3 = \begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix}.
\]

\section*{1. Row View: Dot Products}

Each entry of $ \mathbf{w} = B\mathbf{v} $ is the dot product of a row of $ B $ with $ \mathbf{v} $:
\[
\begin{aligned}
w_1 &= r_1 \cdot \mathbf{v} = (1)(2) + (2)(-1) + (3)(1) = 2 - 2 + 3 = 3, \\
w_2 &= r_2 \cdot \mathbf{v} = (4)(2) + (5)(-1) + (6)(1) = 8 - 5 + 6 = 9, \\
w_3 &= r_3 \cdot \mathbf{v} = (7)(2) + (8)(-1) + (9)(1) = 14 - 8 + 9 = 15.
\end{aligned}
\]

Thus,
\[
\begin{bmatrix}
- & r_1 & - \\
- & r_2 & - \\
- & r_3 & - \\
\end{bmatrix} \mathbf{v}
%\begin{bmatrix}
%2 \\ -1 \\ 1
%\end{bmatrix}
=
\begin{bmatrix}
r_1 \cdot \mathbf{v} \\
r_2 \cdot \mathbf{v} \\
r_3 \cdot \mathbf{v}
\end{bmatrix}
=
\begin{bmatrix}
3 \\ 9 \\ 15
\end{bmatrix}.
\]

\section*{2. Column View: Linear Combination}

The product $ B\mathbf{v} $ is a linear combination of the columns of $ B $, weighted by the entries of $ \mathbf{v} $:
\[
B\mathbf{v} = v_1 c_1 + v_2 c_2 + v_3 c_3 = 2 c_1 + (-1) c_2 + 1 c_3.
\]

Compute:
\[
2 c_1 = \begin{bmatrix} 2 \\ 8 \\ 14 \end{bmatrix}, \quad
-1 c_2 = \begin{bmatrix} -2 \\ -5 \\ -8 \end{bmatrix}, \quad
1 c_3 = \begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix}.
\]

Add them:
\[
\begin{bmatrix} 2 \\ 8 \\ 14 \end{bmatrix}
+
\begin{bmatrix} -2 \\ -5 \\ -8 \end{bmatrix}
+
\begin{bmatrix} 3 \\ 6 \\ 9 \end{bmatrix}
=
\begin{bmatrix}
2 - 2 + 3 \\
8 - 5 + 6 \\
14 - 8 + 9
\end{bmatrix}
=
\begin{bmatrix}
3 \\ 9 \\ 15
\end{bmatrix}.
\]

So,
\[
\begin{bmatrix}
| & | & | \\
c_1 & c_2 & c_3 \\
| & | & | \\
\end{bmatrix}
\begin{bmatrix}
2 \\ -1 \\ 1
\end{bmatrix}
=
2 \begin{bmatrix} | \\ c_1 \\ | \end{bmatrix}
- 1 \begin{bmatrix} | \\ c_2 \\ | \end{bmatrix}
+ 1 \begin{bmatrix} | \\ c_3 \\ | \end{bmatrix}
=
\begin{bmatrix}
3 \\ 9 \\ 15
\end{bmatrix}.
\]

\section*{Conclusion}

Both interpretations yield the same result:
\[
B\mathbf{v} = \begin{bmatrix} 3 \\ 9 \\ 15 \end{bmatrix}.
\]



\section{The Transpose}

Suppose that $A$ is an $m$ by $n$ matrix. This means that $A$ has $m$ rows and $n$ columns and is a map from $\mathbb{R}^n$ to $\mathbb{R}^m$. 

The \emph{transpose} map $A^T$ is defined by: $A^T_{ij} = A_{ji}$. The transpose $A^T$ is a map from $\mathbb{R}^m$ to $\mathbb{R}^n$. It has $n$ rows and $m$ columns. 


\[ A =
\begin{array}{c@{\,}c@{\,}c}
  & n & \\
 &
\left[
\begin{array}{ccc}
 * & *  & *  \\
 * & *  & *  \\
  * & *  & *  \\
   * & *  & *  \\
    * & *  & *  
\end{array}
\right] m
\end{array}
\]
\[
 A^T =
\begin{array}{c@{\,}c@{\,}c}
  & m & \\
 &
\left[
\begin{array}{ccccc}
 *   &  * & * & * & * \\
*   &  * & * & * & * \\
*   &  * & * & * & * 
\end{array}
\right] n
\end{array}
\]

The \emph{image} of $A$ is the subspace of $\mathbb{R}^m$ consisting of vectors of the form $Av$ where $v \in \mathbb{R}^n$. This is also called the \emph{column space} of $A$ and denoted by $\mathrm{Col}(A)$.

The \emph{kernel} of $A$ is the subspace of $\mathbb{R}^n$ consisting of vectors $v$ such that $Av = 0$. This is also called the \emph{null space} of $A$ and denoted by $\mathrm{Nul}(A)$.

To re-iterate:

\[ \mathrm{Col}(A) \subseteq \mathbb{R}^m   \qquad
\mathrm{Nul}(A) \subseteq \mathbb{R}^n
\]

It is important to note that these two subspaces to {\emph not} lie in the same ambiant space. 

By similar reasoning the \emph{image} of $A^T$ is the subspace of $\mathbb{R}^n$ consisting of vectors of the form $A^Tv$ where $v \in \mathbb{R}^m$. This is also called the \emph{column space} of $A^T$ and denoted by $\mathrm{Col}(A^T)$. This subspace is sometimes also called the \emph{row space} of $A$.

The \emph{kernel} of $A^T$ is the subspace of $\mathbb{R}^m$ consisting of vecors $v$ such that $A^Tv = 0$. This is also called the \emph{null space} of $A^T$ and denoted by $\mathrm{Nul}(A^T)$. This subspace is sometimes called the \emph{annihilator} of $\mathrm{Col}(A)$




We have:

\[ \mathrm{Col}(A^T) \subseteq \mathbb{R}^n   \qquad
\mathrm{Nul}(A^T) \subseteq \mathbb{R}^m
\]

Both $\mathrm{Col}(A^T)$ and $\mathrm{Nul}(A)$ lie in $\mathbb{R}^n$. And both $\mathrm{Col}(A)$ and $\mathrm{Nul}(A^T)$ lie in $R^m$

The main result of this section is the following:

\begin{theorem}
The four fundamental subspaces satisfy the following orthogonal decompositions:
\[
\mathbb{R}^n = \mathrm{Col}(A^T) \oplus \mathrm{Nul}(A), \qquad
\mathbb{R}^m = \mathrm{Col}(A) \oplus \mathrm{Nul}(A^\top)
\]
where \( \oplus \) denotes an orthogonal direct sum.

\begin{proof}
We must show that:
\[
\boxed{\mathrm{Nul}(A) = \mathrm{Col}(A^T)^\perp} \quad \text{and} \quad \boxed{\mathrm{Nul}(A^\top) = \mathrm{Col}(A)^\perp}
\]

Note that by symmetry we only really need to prove one of these identities. We will prove the one on the left. 

Note firstly that the columns of $A^T$ are simply the rows of $A$.
So what we will actually prove is that:

\[ \mathrm{Nul}(A) = \mathrm{Row}(A)^\perp\]

We shall show that a vector \( \mathbf{x} \in \mathbb{R}^n \) satisfies \( A\mathbf{x} = \mathbf{0} \) if and only if \( \mathbf{x} \) is orthogonal to every vector in the row space of \( A \).

Let the rows of \( A \) be \( \mathbf{r}_1, \mathbf{r}_2, \dots, \mathbf{r}_m \in \mathbb{R}^n \). Then:
\[
A\mathbf{x} = 
\begin{bmatrix}
\mathbf{r}_1 \cdot \mathbf{x} \\
\mathbf{r}_2 \cdot \mathbf{x} \\
\vdots \\
\mathbf{r}_m \cdot \mathbf{x}
\end{bmatrix}
\]

Thus,
\[
A\mathbf{x} = \mathbf{0} \iff \mathbf{r}_i \cdot \mathbf{x} = 0 \quad \text{for all } i = 1, \dots, m
\]

Any vector \( \mathbf{v} \in \mathrm{Row}(A) \) is a linear combination:
\[
\mathbf{v} = c_1 \mathbf{r}_1 + \cdots + c_m \mathbf{r}_m
\]

Then:
\[
\mathbf{v} \cdot \mathbf{x} = \sum_{i=1}^m c_i (\mathbf{r}_i \cdot \mathbf{x}) = 0
\]

So \( \mathbf{x} \perp \mathbf{v} \), hence \( \mathbf{x} \in \mathrm{Row}(A)^\perp \).

Conversely, if \( \mathbf{x} \in \mathrm{Row}(A)^\perp \), then in particular \( \mathbf{x} \perp \mathbf{r}_i \) for each row \( \mathbf{r}_i \), so \( A\mathbf{x} = \mathbf{0} \), meaning \( \mathbf{x} \in \mathrm{Nul}(A) \).

Therefore:
\[
\boxed{\mathrm{Nul}(A) = \mathrm{Row}(A)^\perp}
\]



\end{proof}


\end{theorem}






















\newpage


\section*{The Dual Space}

Hopefully the reader has seen the idea of the \emph{dual space} before. Let $ V $ be a finite dimensional vector space over the real numbers $ \mathbb{R} $  
The \textbf{dual space} of $ V $, denoted $ V^* $, is the set of all \textbf{linear functionals} on $ V $. That is,
\[
V^* = \{ f: V \to \mathbb{R} \mid f \text{ is linear} \}.
\]

A \textbf{linear functional} is a function $ f: V \to \mathbb{R} $ such that for all vectors $ \mathbf{u}, \mathbf{v} \in V $ and scalars $ \alpha, \beta \in \mathbb{R} $,
\[
f(\alpha \mathbf{u} + \beta \mathbf{v}) = \alpha f(\mathbf{u}) + \beta f(\mathbf{v}).
\]

The dual space $ V^* $ is itself a vector space over $ \mathbb{F} $, with vector addition and scalar multiplication defined \textit{pointwise}:
\begin{align*}
(f + g)(\mathbf{v}) &= f(\mathbf{v}) + g(\mathbf{v}), \\
(\alpha f)(\mathbf{v}) &= \alpha \, f(\mathbf{v}),
\end{align*}
for all $ f, g \in V^* $, $ \alpha \in \mathbb{R} $, and $ \mathbf{v} \in V $.

\section*{Key Properties (Finite-Dimensional Case)}

\begin{lemma}
$\dim V^* = \dim V$ for finite-dimensional $V$

\begin{proof}

Let $V$ be a finite-dimensional vector space over a field $\mathbb{R}$, and let $\dim V = n$.  
Choose a basis $\{e_1, e_2, \dots, e_n\}$ for $V$.

Define linear functionals $e^1, e^2, \dots, e^n \in V^* = \operatorname{Hom}(V, \mathbb{R})$ by
\[
e^i(e_j) = \delta^i_j =
\begin{cases}
1 & \text{if } i = j, \\
0 & \text{if } i \ne j,
\end{cases}
\quad \text{for all } 1 \le i,j \le n.
\]

We claim that $\{e^1, \dots, e^n\}$ is a basis for $V^*$.

\medskip

\noindent\textbf{Linear independence:}  
Suppose $\sum_{i=1}^n a_i e^i = 0$ in $V^*$, where $a_i \in \mathbb{R}$.  
Applying both sides to $e_j$ gives
\[
0 = \left( \sum_{i=1}^n a_i e^i \right)(e_j) = \sum_{i=1}^n a_i e^i(e_j) = \sum_{i=1}^n a_i \delta^i_j = a_j.
\]
Thus $a_j = 0$ for all $j$, so the set $\{e^1, \dots, e^n\}$ is linearly independent.

\medskip

\noindent\textbf{Spanning:}  
Let $f \in V^*$ be arbitrary. Define scalars $c_i = f(e_i) \in \mathbb{R}$ for $i = 1, \dots, n$, and consider the functional
\[
g = \sum_{i=1}^n c_i e^i \in V^*.
\]
For any basis vector $e_j$, we have
\[
g(e_j) = \sum_{i=1}^n c_i e^i(e_j) = \sum_{i=1}^n c_i \delta^i_j = c_j = f(e_j).
\]
Since $f$ and $g$ agree on a basis of $V$, they agree on all of $V$; hence $f = g$.  
Therefore, every $f \in V^*$ is a linear combination of $\{e^1, \dots, e^n\}$, so this set spans $V^*$.

\medskip

Since $\{e^1, \dots, e^n\}$ is a basis for $V^*$, we conclude that
\[
\dim V^* = n = \dim V.
\]

\end{proof}
\end{lemma}








As a consequence of the above, if $ \{ \mathbf{e}_1, \dots, \mathbf{e}_n \} $ is a basis for $ V $, there is a uniquely associated \textbf{dual basis} $ \{ \mathbf{e}^1, \dots, \mathbf{e}^n \} \subseteq V^* $ defined by
    \[
    \mathbf{e}^i(\mathbf{e}_j) = \delta^i_j =
    \begin{cases}
    1 & \text{if } i = j, \\
    0 & \text{if } i \ne j.
    \end{cases}
    \]
    
    Every linear functional $ f \in V^* $ can be uniquely expressed as
    \[
    f = \sum_{i=1}^n f(\mathbf{e}_i) \, \mathbf{e}^i.
    \]



In $ \mathbb{R}^n $ and $ \mathbb{R}^m $, the dot product identifies vectors with linear functionals:
\[
\mathbf{y} \in \mathbb{R}^m \quad \longleftrightarrow \quad f_{\mathbf{y}}(\mathbf{z}) = \mathbf{y}^\top \mathbf{z} \in (\mathbb{R}^m)^*.
\]







\section*{Some intuition}

It can sometimes help to think of vectors in $V$ as column vectors and vectors in $V^*$ as row vectors. The dot product is now the application of a linear functional to a vector. The transpose is the isomoprhism between $V$ and $V^*$.



\section*{The Adjoint}

The reason why we have gone to all the trouble of introducing the dual space is that the transpose of a matrix is the natural matrix representation of the \textbf{adjoint} of a linear map. 

\section*{1. The Adjoint of a Linear Map}

Let $ T: V \to W $ be a linear map between finite-dimensional real vector spaces.  
The \textbf{dual map} $ T^*: W^* \to V^* $ is defined by
\[
(T^* f)(\mathbf{v}) = f(T\mathbf{v}) \quad \text{for all } f \in W^*,\ \mathbf{v} \in V.
\]

In words: to evaluate $ T^* f $ at a vector $ \mathbf{v} $, first apply $ T $ to $ \mathbf{v} $, then apply the functional $ f $ to the result.  

\section*{2. Matrix Representation of adjoint}

Choose bases:

\begin{itemize}
\item $ \mathcal{B} = \{ \mathbf{v}_1, \dots, \mathbf{v}_n \} $ for $ V $,
    \item $ \mathcal{C} = \{ \mathbf{w}_1, \dots, \mathbf{w}_m \} $ for $ W $,
    \item $ \mathcal{B}^* = \{ \mathbf{v}^1, \dots, \mathbf{v}^n \} $ and $ \mathcal{C}^* = \{ \mathbf{w}^1, \dots, \mathbf{w}^m \} $ for the dual bases.
\end{itemize}


Suppose the matrix of $ T $ with respect to $ \mathcal{B}, \mathcal{C} $ is $ A = (a_{ij})$, so
Then:
\[
T(\mathbf{v}_1) = \sum_{i=1}^{m} A_{i1} \, \mathbf{w}_i
\]

More generally:
\[
T(\mathbf{v}_j) = \sum_{i=1}^m a_{ij} \mathbf{w}_i.
\]

We compute the matrix of $ T^* $ with respect to $ \mathcal{C}^*, \mathcal{B}^* $.  
For any $ k \in \{1, \dots, m\} $ and $ j \in \{1, \dots, n\} $,
\[
(T^* \mathbf{w}^k)(\mathbf{v}_j) = \mathbf{w}^k(T \mathbf{v}_j) = \mathbf{w}^k\left( \sum_{i=1}^m a_{ij} \mathbf{w}_i \right) = a_{kj}.
\]

On the other hand, if the matrix of $ T^* $ is $ B = (b_{\ell k}) $, then
\[
\boxed{T^*(\mathbf{w}^1) = \sum_{\ell=1}^{n} b_{\ell 1} \, \mathbf{v}^\ell}
\]


More generally:
\[
T^*(\mathbf{w}^k) = \sum_{\ell=1}^n b_{\ell k} \mathbf{v}^\ell,
\]
so
\[
(T^* \mathbf{w}^k)(\mathbf{v}_j) = b_{j k}.
\]

Comparing both expressions gives $ b_{j k} = a_{k j} $, so $ B = A^\top $.

\begin{center}
\textbf{The matrix of the dual map $ T^* $ is the transpose of the matrix of $ T $.}
\end{center}













\section*{The Annihilator}

The \textbf{annihilator} provides the abstract, coordinate-free foundation for the orthogonal relationships among the four fundamental subspaces. It explains \textit{why} these subspaces come in perpendicular pairs—and reveals that this structure is not just a coincidence of matrices, but a deep property of vector spaces and duality.

\section*{1. The Four Fundamental Subspaces}

For a matrix $ A \in \mathbb{R}^{m \times n} $, the four fundamental subspaces are:

\begin{center}
\begin{tabular}{lll}
Subspace & Location & Description \\
\midrule
$ \operatorname{Col}(A) $ & $ \mathbb{R}^m $ & Column space (range) \\
$ \operatorname{Nul}(A^\top) $ & $ \mathbb{R}^m $ & Left null space \\
$ \operatorname{Row}(A) = \operatorname{Col}(A^\top) $ & $ \mathbb{R}^n $ & Row space \\
$ \operatorname{Nul}(A) $ & $ \mathbb{R}^n $ & Null space (kernel)
\end{tabular}
\end{center}

The key orthogonal decompositions are:
\[
\mathbb{R}^n = \operatorname{Row}(A) \oplus \operatorname{Nul}(A), \quad
\mathbb{R}^m = \operatorname{Col}(A) \oplus \operatorname{Nul}(A^\top),
\]
with
\[
\operatorname{Nul}(A) = \operatorname{Row}(A)^\perp, \quad
\operatorname{Nul}(A^\top) = \operatorname{Col}(A)^\perp.
\]

\section*{2. What Is the Annihilator?}

Let $ V $ be a finite-dimensional vector space, and let $ W \subseteq V $ be a subspace.  
The \textbf{annihilator} of $ W $ is:
\[
W^\circ = \{ f \in V^* : f(\mathbf{w}) = 0 \text{ for all } \mathbf{w} \in W \},
\]
where $ V^* $ is the \textbf{dual space} (the space of linear functionals on $ V $).

\begin{itemize}
\item $ W^\circ $ is a subspace of $ V^* $,
    \item $ \dim(W^\circ) = \dim(V) - \dim(W) $.
\end{itemize}

\section*{Dimension Theorem for Annihilators}

Let $V$ be a finite-dimensional vector space over a field $\mathbb{R}$, and let $W \subseteq V$ be a subspace.  
The \textbf{annihilator} of $W$ is defined as
\[
W^\circ = \{ f \in V^* : f(\mathbf{w}) = 0 \text{ for all } \mathbf{w} \in W \}.
\]

We prove that $W^\circ$ is a subspace of $V^*$ and that
\[
\dim(W^\circ) = \dim(V) - \dim(W).
\]

\medskip

\noindent\textbf{Step 1: $W^\circ$ is a subspace of $V^*$.}  
Clearly $0 \in W^\circ$. If $f,g \in W^\circ$ and $\alpha,\beta \in \mathbb{R}$, then for any $\mathbf{w} \in W$,
\[
(\alpha f + \beta g)(\mathbf{w}) = \alpha f(\mathbf{w}) + \beta g(\mathbf{w}) = 0,
\]
so $\alpha f + \beta g \in W^\circ$. Hence $W^\circ \leq V^*$.

\medskip

\noindent\textbf{Step 2: Choose a basis adapted to $W$.}  
Let $\dim V = n$ and $\dim W = k$. Choose a basis
\[
\{ \mathbf{w}_1, \dots, \mathbf{w}_k \}
\]
for $W$, and extend it to a basis of $V$:
\[
\mathcal{B} = \{ \mathbf{w}_1, \dots, \mathbf{w}_k, \mathbf{v}_{k+1}, \dots, \mathbf{v}_n \}.
\]

Let $\mathcal{B}^* = \{ \mathbf{w}^1, \dots, \mathbf{w}^k, \mathbf{v}^{k+1}, \dots, \mathbf{v}^n \}$ be the dual basis of $V^*$, so that
\[
\mathbf{w}^i(\mathbf{w}_j) = \delta^i_j, \quad
\mathbf{v}^i(\mathbf{v}_j) = \delta^i_j, \quad
\mathbf{w}^i(\mathbf{v}_j) = 0, \quad
\mathbf{v}^i(\mathbf{w}_j) = 0
\]
for all valid indices.

\medskip

\noindent\textbf{Step 3: Characterize $W^\circ$ using the dual basis.}  
Let $f \in V^*$. Write $f$ in the dual basis:
\[
f = \sum_{i=1}^k a_i \mathbf{w}^i + \sum_{j=k+1}^n b_j \mathbf{v}^j.
\]
For any $\mathbf{w} \in W$, we have $\mathbf{w} = \sum_{i=1}^k c_i \mathbf{w}_i$, so
\[
f(\mathbf{w}) = \sum_{i=1}^k a_i c_i.
\]
Thus $f(\mathbf{w}) = 0$ for all $\mathbf{w} \in W$ if and only if $a_1 = \cdots = a_k = 0$.

Therefore,
\[
W^\circ = \operatorname{span}\{ \mathbf{v}^{k+1}, \dots, \mathbf{v}^n \}.
\]

\medskip

\noindent\textbf{Step 4: Compute the dimension.}  
The set $\{ \mathbf{v}^{k+1}, \dots, \mathbf{v}^n \}$ is linearly independent and spans $W^\circ$, so it is a basis. Hence
\[
\dim(W^\circ) = n - k = \dim(V) - \dim(W).
\]











\section*{3. Connecting Annihilators to the Four Subspaces}

In $ \mathbb{R}^n $ and $ \mathbb{R}^m $, the dot product identifies vectors with linear functionals:
\[
\mathbf{y} \in \mathbb{R}^m \quad \longleftrightarrow \quad f_{\mathbf{y}}(\mathbf{z}) = \mathbf{y}^\top \mathbf{z} \in (\mathbb{R}^m)^*.
\]

Under this identification:
\[
\operatorname{Nul}(A^\top) \cong (\operatorname{Col}(A))^\circ, \qquad
\operatorname{Nul}(A) \cong (\operatorname{Row}(A))^\circ.
\]





\section*{Annihilators and Null Spaces via the Dot Product}

In $\mathbb{R}^n$ and $\mathbb{R}^m$, the standard dot product identifies vectors with linear functionals:
\[
\mathbf{y} \in \mathbb{R}^m \quad \longleftrightarrow \quad f_{\mathbf{y}} \in (\mathbb{R}^m)^*, \qquad 
f_{\mathbf{y}}(\mathbf{z}) = \mathbf{y}^\top \mathbf{z} = \mathbf{y} \cdot \mathbf{z}.
\]
This is an isomorphism $\mathbb{R}^m \xrightarrow{\sim} (\mathbb{R}^m)^*$, since the dot product is nondegenerate.

Let $A$ be an $m \times n$ real matrix. We prove:
\[
\nul(A^\top) \cong (\col(A))^\circ, \qquad
\nul(A) \cong (\row(A))^\circ,
\]
where the annihilator is taken inside the appropriate dual space, and the isomorphism is the one induced by the dot product.

\medskip

\noindent\textbf{1. Proof that $\nul(A^\top) \cong (\col(A))^\circ$.}

Recall:
\[
\col(A) = \{ A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n \} \subseteq \mathbb{R}^m.
\]
Its annihilator (in $(\mathbb{R}^m)^*$) is
\[
(\col(A))^\circ = \{ f \in (\mathbb{R}^m)^* : f(\mathbf{y}) = 0 \text{ for all } \mathbf{y} \in \col(A) \}.
\]

Under the dot product identification, a functional $f \in (\mathbb{R}^m)^*$ corresponds to a unique vector $\mathbf{v} \in \mathbb{R}^m$ such that $f(\mathbf{y}) = \mathbf{v}^\top \mathbf{y}$ for all $\mathbf{y} \in \mathbb{R}^m$.

Thus, $\mathbf{v} \in \mathbb{R}^m$ corresponds to an element of $(\col(A))^\circ$ iff
\[
\mathbf{v}^\top (A\mathbf{x}) = 0 \quad \text{for all } \mathbf{x} \in \mathbb{R}^n.
\]
But $\mathbf{v}^\top A \mathbf{x} = (A^\top \mathbf{v})^\top \mathbf{x}$, so this holds for all $\mathbf{x}$ iff $A^\top \mathbf{v} = \mathbf{0}$.

Hence,
\[
\mathbf{v} \in \nul(A^\top) \quad \Longleftrightarrow \quad f_{\mathbf{v}} \in (\col(A))^\circ.
\]
Therefore, the dot product isomorphism restricts to an isomorphism
\[
\nul(A^\top) \xrightarrow{\sim} (\col(A))^\circ.
\]

\medskip

\noindent\textbf{2. Proof that $\nul(A) \cong (\row(A))^\circ$.}

Note that $\row(A) = \col(A^\top) \subseteq \mathbb{R}^n$. Applying the previous result to $A^\top$ (which is $n \times m$), we get:
\[
\nul((A^\top)^\top) = \nul(A) \cong (\col(A^\top))^\circ = (\row(A))^\circ,
\]
where the annihilator is now in $(\mathbb{R}^n)^*$.

Alternatively, argue directly:  
$\mathbf{x} \in \mathbb{R}^n$ corresponds to $f_{\mathbf{x}} \in (\mathbb{R}^n)^*$ via $f_{\mathbf{x}}(\mathbf{z}) = \mathbf{x}^\top \mathbf{z}$.  
Then $f_{\mathbf{x}} \in (\row(A))^\circ$ iff $f_{\mathbf{x}}(\mathbf{r}) = 0$ for every row vector $\mathbf{r}$ of $A$ (viewed as elements of $\mathbb{R}^n$).  
Since any $\mathbf{r} \in \row(A)$ is a linear combination of the rows of $A$, this is equivalent to
\[
\mathbf{r}_i \mathbf{x} = 0 \quad \text{for each row } \mathbf{r}_i \text{ of } A,
\]
which is precisely $A\mathbf{x} = \mathbf{0}$, i.e., $\mathbf{x} \in \nul(A)$.

Thus, $\nul(A) \cong (\row(A))^\circ$.

\medskip

\noindent\textbf{Conclusion.}  
Via the standard dot product identification $\mathbb{R}^k \cong (\mathbb{R}^k)^*$, we have natural isomorphisms:
\[
\boxed{\nul(A^\top) \cong (\col(A))^\circ}, \qquad
\boxed{\nul(A) \cong (\row(A))^\circ}.
\]





\section*{4. The Big Picture: Duality}

The annihilator reveals that the four subspaces reflect a fundamental duality.  
The matrix $ A: \mathbb{R}^n \to \mathbb{R}^m $ induces a \textbf{dual map} $ A^*: (\mathbb{R}^m)^* \to (\mathbb{R}^n)^* $, and we have:
\[
\ker(A^*) = (\operatorname{Im} A)^\circ, \qquad \operatorname{Im}(A^*) = (\ker A)^\circ.
\]

When we identify $ (\mathbb{R}^k)^* \cong \mathbb{R}^k $ via the dot product, these become:
\[
\operatorname{Nul}(A^\top) = \operatorname{Col}(A)^\perp, \qquad \operatorname{Row}(A) = \operatorname{Nul}(A)^\perp.
\]

\section*{5. Significance}

\begin{enumerate}
    \item \textbf{Unification}: The annihilator explains both orthogonal decompositions as instances of a single principle: the kernel of a linear map and the annihilator of its image are naturally paired.
    
    \item \textbf{Coordinate-free insight}: The orthogonal relationships hold in any finite-dimensional inner product space—not just for matrices.
    
    \item \textbf{Generalization}: In infinite-dimensional spaces (e.g., Hilbert spaces), the same duality holds via the Riesz representation theorem.
    
    \item \textbf{Conceptual clarity}: The ``four subspaces'' are really two pairs of dual objects:
    \item Domain side: $ \operatorname{Row}(A) $ and $ \operatorname{Nul}(A) $,
        \item Codomain side: $ \operatorname{Col}(A) $ and $ \operatorname{Nul}(A^\top) $.
\end{enumerate}

\section*{Conclusion}

The annihilator is the abstract mechanism that explains why the four fundamental subspaces form orthogonal complements. It reveals that:
\[
\operatorname{Nul}(A) \text{ is the annihilator of } \operatorname{Row}(A), \quad
\operatorname{Nul}(A^\top) \text{ is the annihilator of } \operatorname{Col}(A).
\]
Without the concept of the annihilator, these orthogonal relationships appear as computational coincidences; with it, they emerge as inevitable consequences of linear duality.












\section*{Theorem (Rank--Nullity Theorem)}

Let $ T: V \to W $ be a linear transformation between finite-dimensional vector spaces over a field $ \mathbb{F} $. Then
\[
\dim(V) = \dim(\ker T) + \dim(\operatorname{Im} T).
\]

In particular, for any $ m \times n $ matrix $ A $ over $ \mathbb{F} $, viewing $ A $ as a linear map $ \mathbb{F}^n \to \mathbb{F}^m $, we have
\[
n = \operatorname{nullity}(A) + \operatorname{rank}(A).
\]

\section*{Proof}

Let $ K = \ker T \subseteq V $. Since $ V $ is finite-dimensional, so is $ K $. Let
\[
\{ \mathbf{u}_1, \dots, \mathbf{u}_k \}
\]
be a basis for $ K $, so $ k = \dim(\ker T) $.

Extend this to a basis for all of $ V $:
\[
\{ \mathbf{u}_1, \dots, \mathbf{u}_k, \mathbf{v}_1, \dots, \mathbf{v}_r \},
\]
so that $ \dim(V) = k + r $.

We claim that the set
\[
\{ T(\mathbf{v}_1), \dots, T(\mathbf{v}_r) \}
\]
is a basis for $ \operatorname{Im} T $.

\subsection*{1. Spanning}

Let $ \mathbf{w} \in \operatorname{Im} T $. Then $ \mathbf{w} = T(\mathbf{x}) $ for some $ \mathbf{x} \in V $. Write
\[
\mathbf{x} = \sum_{i=1}^k a_i \mathbf{u}_i + \sum_{j=1}^r b_j \mathbf{v}_j.
\]
Applying $ T $, and using $ T(\mathbf{u}_i) = \mathbf{0} $ (since $ \mathbf{u}_i \in \ker T $), we get
\[
T(\mathbf{x}) = \sum_{j=1}^r b_j T(\mathbf{v}_j).
\]
Thus, $ \{ T(\mathbf{v}_1), \dots, T(\mathbf{v}_r) \} $ spans $ \operatorname{Im} T $.

\subsection*{2. Linear Independence}

Suppose
\[
\sum_{j=1}^r c_j T(\mathbf{v}_j) = \mathbf{0}.
\]
Then
\[
T\left( \sum_{j=1}^r c_j \mathbf{v}_j \right) = \mathbf{0},
\]
so $ \sum_{j=1}^r c_j \mathbf{v}_j \in \ker T = \operatorname{span}\{ \mathbf{u}_1, \dots, \mathbf{u}_k \} $.

But the full set $ \{ \mathbf{u}_1, \dots, \mathbf{u}_k, \mathbf{v}_1, \dots, \mathbf{v}_r \} $ is linearly independent.  
Therefore, the only linear combination of the $ \mathbf{v}_j $'s that lies in $ \operatorname{span}\{ \mathbf{u}_1, \dots, \mathbf{u}_k \} $ is the trivial one.  
Hence, $ c_j = 0 $ for all $ j $, and the set $ \{ T(\mathbf{v}_1), \dots, T(\mathbf{v}_r) \} $ is linearly independent.

\subsection*{3. Conclusion}

We have shown that $ \dim(\operatorname{Im} T) = r $. Since $ \dim(V) = k + r $, it follows that
\[
\dim(V) = \dim(\ker T) + \dim(\operatorname{Im} T).
\]

This completes the proof.

\section*{Remarks}

\begin{itemize}
\item This proof uses no matrices, coordinates, or row reduction.
    \item It works over any field (e.g., $ \mathbb{R} $, $ \mathbb{C} $, finite fields).
    \item It reveals that Rank--Nullity is a structural property of linear maps, not a computational artifact.
\end{itemize}



\section*{Introduction}

A fundamental fact in linear algebra is that the rank of a matrix $ A $ is equal to the rank of its transpose $ A^\top $.  
In other words, the maximum number of linearly independent \textbf{columns} of $ A $ is the same as the maximum number of linearly independent \textbf{rows} of $ A $.

This might seem surprising at first—after all, rows and columns live in different spaces!  
But in $ \mathbb{R}^n $ (or $ \mathbb{C}^n $), the presence of an inner product (the dot product) creates a deep symmetry between rows and columns.

We will prove this result using two key ideas:
\begin{enumerate}
    \item The \textbf{Rank--Nullity Theorem},
    \item The \textbf{orthogonal relationship} between the null space and the row space.
\end{enumerate}

This proof works for real matrices (and complex matrices with minor adjustments), but it relies on the geometry of the dot product.

\section*{Step 1: What the Rank--Nullity Theorem Tells Us}

Let $ A $ be an $ m \times n $ real matrix.  
We can think of $ A $ as a linear transformation that maps vectors from $ \mathbb{R}^n $ to $ \mathbb{R}^m $.

The Rank--Nullity Theorem says:
\[
\text{(dimension of domain)} = \text{(dimension of null space)} + \text{(dimension of column space)}.
\]

In symbols:
\[
n = \dim(\operatorname{Nul}(A)) + \dim(\operatorname{Col}(A)).
\tag{1}
\]

Here:
\begin{itemize}
\item $ \operatorname{Nul}(A) = \{ \mathbf{x} \in \mathbb{R}^n : A\mathbf{x} = \mathbf{0} \} $ is the set of vectors that $ A $ sends to zero,
    \item $ \operatorname{Col}(A) $ is the span of the columns of $ A $, and its dimension is the \textbf{column rank} of $ A $.
\end{itemize}

\section*{Step 2: The Geometric Link Between Rows and Null Space}

Now consider the \textbf{row space} of $ A $, denoted $ \operatorname{Row}(A) $.  
This is the subspace of $ \mathbb{R}^n $ spanned by the rows of $ A $.

Here’s the key geometric insight:

\begin{center}
\textbf{A vector $ \mathbf{x} $ is in the null space of $ A $ if and only if it is perpendicular to every row of $ A $.}
\end{center}

Why? Because the equation $ A\mathbf{x} = \mathbf{0} $ means that the dot product of each row of $ A $ with $ \mathbf{x} $ is zero.

In other words:
\[
\operatorname{Nul}(A) = \{ \mathbf{x} \in \mathbb{R}^n : \mathbf{x} \perp \mathbf{r} \text{ for every row } \mathbf{r} \text{ of } A \}.
\]

This is precisely the definition of the \textbf{orthogonal complement} of the row space. So we have:
\[
\operatorname{Nul}(A) = \operatorname{Row}(A)^\perp.
\tag{2}
\]

\section*{Step 3: Use Dimensions of Orthogonal Complements}

In $ \mathbb{R}^n $, if $ S $ is any subspace, then:
\[
\dim(S) + \dim(S^\perp) = n.
\]

Apply this to $ S = \operatorname{Row}(A) $. Using (2), we get:
\[
\dim(\operatorname{Row}(A)) + \dim(\operatorname{Nul}(A)) = n.
\tag{3}
\]

But $ \dim(\operatorname{Row}(A)) $ is exactly the \textbf{row rank} of $ A $.

\section*{Step 4: Compare with Rank--Nullity}

Now look back at equation (1) from Rank--Nullity:
\[
n = \dim(\operatorname{Nul}(A)) + \dim(\operatorname{Col}(A)).
\]

And equation (3) from orthogonality:
\[
n = \dim(\operatorname{Nul}(A)) + \dim(\operatorname{Row}(A)).
\]

Both right-hand sides equal $ n $, and both contain the term $ \dim(\operatorname{Nul}(A)) $.  
Therefore, the remaining terms must be equal:
\[
\dim(\operatorname{Col}(A)) = \dim(\operatorname{Row}(A)).
\]

In other words:
\[
\text{column rank of } A = \text{row rank of } A.
\]

Since the row rank of $ A $ is the same as the column rank of $ A^\top $, we conclude:
\[
\operatorname{rank}(A) = \operatorname{rank}(A^\top).
\]

\section*{Conclusion}

The equality of row and column rank is not a coincidence—it is a consequence of the **geometric structure** of Euclidean space.  
The dot product creates a perfect pairing between the row space and the null space, and the Rank--Nullity Theorem translates this geometric fact into an algebraic equality of dimensions.

This proof beautifully ties together:
\begin{itemize}
\item Linear algebra (Rank--Nullity),
    \item Geometry (orthogonality),
    \item Matrix theory (rows vs. columns).
\end{itemize}

\end{document}
\newpage


\section*{Four Fundamental Subspaces}

In linear algebra, the \textbf{four fundamental subspaces} associated with a real matrix \( A \in \mathbb{R}^{m \times n} \) provide deep insight into the structure of linear systems. These are:
\begin{enumerate}[label=\arabic*.]
    \item Column Space (\( \mathrm{Col}(A) \))
    \item Null Space (\( \mathrm{Nul}(A) \))
    \item Row Space (\( \mathrm{Row}(A) \))
    \item Annihlator (\( \mathrm{Nul}(A^\top) \))
\end{enumerate}

We will define each subspace and then rigorously prove the key orthogonality relations:
\[
\boxed{\mathrm{Nul}(A) = \mathrm{Row}(A)^\perp} \quad \text{and} \quad \boxed{\mathrm{Nul}(A^\top) = \mathrm{Col}(A)^\perp}
\]

\section*{1. Definitions of the Four Fundamental Subspaces}

Let \( A \) be an \( m \times n \) real matrix.

\subsection*{1.1 Column Space}
The column space of \( A \) is the set of all linear combinations of its columns:
\[
\mathrm{Col}(A) = \left\{ A\mathbf{x} \mid \mathbf{x} \in \mathbb{R}^n \right\} \subseteq \mathbb{R}^m
\]
- Dimension: \( \mathrm{rank}(A) = r \)
- Basis: Pivot columns of \( A \)

\subsection*{1.2 Null Space}
The null space of \( A \) is the set of vectors mapped to zero:
\[
\mathrm{Nul}(A) = \left\{ \mathbf{x} \in \mathbb{R}^n \mid A\mathbf{x} = \mathbf{0} \right\} \subseteq \mathbb{R}^n
\]
- Dimension: \( n - r \) (Rank-Nullity Theorem)
- Basis: Found by solving \( A\mathbf{x} = \mathbf{0} \)

\subsection*{1.3 Row Space}
The row space is the span of the rows of \( A \), or equivalently, the column space of \( A^\top \):
\[
\mathrm{Row}(A) = \mathrm{Col}(A^\top) \subseteq \mathbb{R}^n
\]
- Dimension: \( r \)
- Basis: Nonzero rows in the reduced row echelon form (RREF) of \( A \)

\subsection*{1.4 Left Null Space}
The left null space consists of vectors \( \mathbf{y} \) such that \( A^\top \mathbf{y} = \mathbf{0} \):
\[
\mathrm{Nul}(A^\top) = \left\{ \mathbf{y} \in \mathbb{R}^m \mid A^\top \mathbf{y} = \mathbf{0} \right\} \subseteq \mathbb{R}^m
\]
- Dimension: \( m - r \)
- Basis: Found via elimination on \( A^\top \), or from bottom rows of \( E \) in \( EA = R \)

\section*{2. Orthogonality Relations}

We now prove the two fundamental orthogonal complement relationships.

\subsection*{2.1 \( \mathrm{Nul}(A) = \mathrm{Row}(A)^\perp \)}

We show that a vector \( \mathbf{x} \in \mathbb{R}^n \) satisfies \( A\mathbf{x} = \mathbf{0} \) if and only if \( \mathbf{x} \) is orthogonal to every vector in the row space of \( A \).

Let the rows of \( A \) be \( \mathbf{r}_1, \mathbf{r}_2, \dots, \mathbf{r}_m \in \mathbb{R}^n \). Then:
\[
A\mathbf{x} = 
\begin{bmatrix}
\mathbf{r}_1 \cdot \mathbf{x} \\
\mathbf{r}_2 \cdot \mathbf{x} \\
\vdots \\
\mathbf{r}_m \cdot \mathbf{x}
\end{bmatrix}
\]

Thus,
\[
A\mathbf{x} = \mathbf{0} \iff \mathbf{r}_i \cdot \mathbf{x} = 0 \quad \text{for all } i = 1, \dots, m
\]

Any vector \( \mathbf{v} \in \mathrm{Row}(A) \) is a linear combination:
\[
\mathbf{v} = c_1 \mathbf{r}_1 + \cdots + c_m \mathbf{r}_m
\]

Then:
\[
\mathbf{v} \cdot \mathbf{x} = \sum_{i=1}^m c_i (\mathbf{r}_i \cdot \mathbf{x}) = 0
\]

So \( \mathbf{x} \perp \mathbf{v} \), hence \( \mathbf{x} \in \mathrm{Row}(A)^\perp \).

Conversely, if \( \mathbf{x} \in \mathrm{Row}(A)^\perp \), then in particular \( \mathbf{x} \perp \mathbf{r}_i \) for each row \( \mathbf{r}_i \), so \( A\mathbf{x} = \mathbf{0} \), meaning \( \mathbf{x} \in \mathrm{Nul}(A) \).

Therefore:
\[
\boxed{\mathrm{Nul}(A) = \mathrm{Row}(A)^\perp}
\]

\subsection*{2.2 \( \mathrm{Nul}(A^\top) = \mathrm{Col}(A)^\perp \)}

Now let \( \mathbf{y} \in \mathbb{R}^m \). Consider \( A^\top \mathbf{y} \). The \( j \)-th entry of \( A^\top \mathbf{y} \) is:
\[
(A^\top \mathbf{y})_j = \text{(column } j \text{ of } A) \cdot \mathbf{y} = \mathbf{a}_j \cdot \mathbf{y}
\]

So:
\[
A^\top \mathbf{y} = \mathbf{0} \iff \mathbf{a}_j \cdot \mathbf{y} = 0 \quad \text{for all } j = 1, \dots, n
\]

That is, \( \mathbf{y} \) is orthogonal to every column of \( A \).

Any vector \( \mathbf{w} \in \mathrm{Col}(A) \) is of the form:
\[
\mathbf{w} = d_1 \mathbf{a}_1 + \cdots + d_n \mathbf{a}_n
\]

Then:
\[
\mathbf{y} \cdot \mathbf{w} = \sum_{j=1}^n d_j (\mathbf{y} \cdot \mathbf{a}_j) = 0
\]

So \( \mathbf{y} \perp \mathbf{w} \), hence \( \mathbf{y} \in \mathrm{Col}(A)^\perp \).

Conversely, if \( \mathbf{y} \in \mathrm{Col}(A)^\perp \), then \( \mathbf{y} \cdot \mathbf{a}_j = 0 \) for all \( j \), so \( A^\top \mathbf{y} = \mathbf{0} \), so \( \mathbf{y} \in \mathrm{Nul}(A^\top) \).

Thus:
\[
\boxed{\mathrm{Nul}(A^\top) = \mathrm{Col}(A)^\perp}
\]

\section*{3. Summary}

The four fundamental subspaces satisfy the following orthogonal decompositions:
\[
\mathbb{R}^n = \mathrm{Row}(A) \oplus \mathrm{Nul}(A), \qquad
\mathbb{R}^m = \mathrm{Col}(A) \oplus \mathrm{Nul}(A^\top)
\]
where \( \oplus \) denotes an orthogonal direct sum.

These results are central to the \textbf{Fundamental Theorem of Linear Algebra}, illustrating how a matrix partitions the domain and codomain into mutually orthogonal invariant subspaces.

\end{document}
